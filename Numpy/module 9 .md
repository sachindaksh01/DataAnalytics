


---

# ğŸ“˜ Module 9: Linear Algebra with NumPy
Linear algebra is **the backbone of data science, machine learning, and computer graphics**. NumPy provides efficient functions to handle **matrix operations**, which are essential for ML algorithms, data transformations, and analytics.

---

## 1. Dot Product â€“ `np.dot()`

ğŸ‘‰ **Definition:** The dot product is the sum of products of corresponding elements.
ğŸ‘‰ **Use in analytics:** Used in calculating **similarity (cosine similarity), projections, and ML weights**.

### Example: Student Scores and Weightage

```python
import numpy as np

scores = np.array([80, 90, 70])       # marks in 3 subjects
weights = np.array([0.3, 0.4, 0.3])   # weightage of each subject

final_score = np.dot(scores, weights)
print("Weighted Final Score:", final_score)
```

**Output:**

```
Weighted Final Score: 81.0
```

ğŸ“Œ **Interpretation:** The studentâ€™s overall score = 81 after applying subject weightages.

---

## 2. Matrix Multiplication â€“ `np.matmul()`

ğŸ‘‰ **Definition:** Multiplication of two matrices (row Ã— column).
ğŸ‘‰ **Use in analytics:** Transformations, combining datasets, ML algorithms (e.g., neural networks).

### Example: Sales Data Transformation

```python
sales = np.array([[2,3],   # 2 apples, 3 bananas
                  [1,4]])  # 1 apple, 4 bananas
prices = np.array([[50],   # price of apple
                   [30]])  # price of banana

total = np.matmul(sales, prices)
print("Total Sales per Customer:\n", total)
```

**Output:**

```
Total Sales per Customer:
 [[190]
  [170]]
```

ğŸ“Œ **Interpretation:**

* Customer 1 â†’ 2Ã—50 + 3Ã—30 = 190
* Customer 2 â†’ 1Ã—50 + 4Ã—30 = 170

---

## 3. Transpose â€“ `.T`

ğŸ‘‰ **Definition:** Swaps rows with columns.
ğŸ‘‰ **Use in analytics:** Used in **linear equations, dataset reshaping, ML optimization**.

### Example: Student Marks Table

```python
marks = np.array([[80, 90, 70],  # Student 1
                  [85, 95, 75]]) # Student 2

print("Original:\n", marks)
print("Transposed:\n", marks.T)
```

**Output:**

```
Original:
 [[80 90 70]
 [85 95 75]]

Transposed:
 [[80 85]
 [90 95]
 [70 75]]
```

ğŸ“Œ **Interpretation:** Now each row represents **subject-wise marks** instead of student-wise.

---

## 4. Inverse Matrix â€“ `np.linalg.inv()`

ğŸ‘‰ **Definition:** A matrix multiplied by its inverse gives the identity matrix.
ğŸ‘‰ **Use in analytics:** Solving systems of linear equations (used in regression, optimization).

### Example: Solving Equations

Suppose we have equations:

```
2x + y = 8
3x + 4y = 18
```

```python
A = np.array([[2,1],
              [3,4]])
B = np.array([8,18])

X = np.linalg.inv(A).dot(B)
print("Solutions (x, y):", X)
```

**Output:**

```
Solutions (x, y): [2. 4.]
```

ğŸ“Œ **Interpretation:** Equation solved â†’ x = 2, y = 4.

---

## 5. Determinant â€“ `np.linalg.det()`

ğŸ‘‰ **Definition:** A scalar value representing a matrixâ€™s properties.
ğŸ‘‰ **Use in analytics:**

* If det = 0 â†’ matrix is **singular** (no inverse).
* Used in transformations, stability analysis.

### Example:

```python
A = np.array([[2,1],
              [3,4]])

det = np.linalg.det(A)
print("Determinant:", det)
```

**Output:**

```
Determinant: 5.000000000000001
```

ğŸ“Œ **Interpretation:** Since determinant â‰  0, the matrix has an inverse.

---

## 6. Eigenvalues and Eigenvectors â€“ `np.linalg.eig()`

ğŸ‘‰ **Definition:**

* **Eigenvalues** show how much variance/direction a vector explains.
* **Eigenvectors** are the directions of maximum variance.
  ğŸ‘‰ **Use in analytics:** Very important in **PCA (Principal Component Analysis)** for dimensionality reduction in ML.

### Example:

```python
A = np.array([[4,2],
              [1,3]])

values, vectors = np.linalg.eig(A)
print("Eigenvalues:", values)
print("Eigenvectors:\n", vectors)
```

**Possible Output:**

```
Eigenvalues: [5. 2.]
Eigenvectors:
 [[ 0.89442719 -0.70710678]
 [ 0.4472136   0.70710678]]
```

ğŸ“Œ **Interpretation:**

* Eigenvalues â†’ [5,2] (show strength of directions).
* Eigenvectors â†’ directions of variance.
* Used in ML (PCA) to reduce dataset from **high dimension â†’ fewer features**.

---

# ğŸ”‘ Summary

* **Dot product (`dot`)** â†’ Weighted sums (e.g., final score, similarity).
* **Matrix multiplication (`matmul`)** â†’ Transform datasets, sales calculations, ML models.
* **Transpose (`.T`)** â†’ Row â†” Column switch, useful in reshaping data.
* **Inverse (`inv`)** â†’ Solve equations, regression models.
* **Determinant (`det`)** â†’ Matrix stability, check if inverse exists.
* **Eigenvalues & Eigenvectors (`eig`)** â†’ Explain variance, used in PCA, ML feature reduction.

ğŸ‘‰ Linear Algebra with NumPy is the **foundation for Data Analytics, Machine Learning, and AI models**.

---
Great âœ…
Hereâ€™s a **10-question practice set** for **Module 9: Linear Algebra with NumPy**.
All problems are **real-world style** so students can connect them easily.

---

# ğŸ“ Practice Set â€“ Linear Algebra with NumPy (10 Questions)

---

### **Q1. Dot Product â€“ Weighted Average**

A student scored `[80, 90, 70]` in 3 subjects with weightages `[0.3, 0.4, 0.3]`.
ğŸ‘‰ Use `np.dot()` to calculate the **final weighted score**.

---

### **Q2. Dot Product â€“ Salary Components**

An employee has `[40, 20, 10]` hours in different tasks with hourly rates `[50, 100, 150]`.
ğŸ‘‰ Find total salary using `np.dot()`.

---

### **Q3. Matrix Multiplication â€“ Sales Calculation**

A store sells **apples and bananas**.

* Customer purchases (matrix):

```
[[2, 3],  # Customer 1 â†’ 2 apples, 3 bananas
 [1, 4]]  # Customer 2 â†’ 1 apple, 4 bananas
```

* Prices (matrix):

```
[[50],  # Apple price
 [30]]  # Banana price
```

ğŸ‘‰ Use `np.matmul()` to find each customerâ€™s total bill.

---

### **Q4. Matrix Multiplication â€“ Student Marks**

Two students gave tests:

```
Marks = [[80, 90], [70, 60]]  
Weights = [[0.6, 0.4], [0.5, 0.5]]  
```

ğŸ‘‰ Multiply to get **weighted marks** for each student.

---

### **Q5. Transpose â€“ Student Marks Table**

Marks of 3 students in 2 subjects:

```
[[80, 85],
 [90, 95],
 [70, 75]]
```

ğŸ‘‰ Use transpose (`.T`) to view subject-wise marks.

---

### **Q6. Inverse Matrix â€“ Solve Equations**

Solve the system of equations using `np.linalg.inv()`:

```
2x + y = 8  
3x + 4y = 18
```

---

### **Q7. Determinant â€“ Matrix Validity**

Check determinant of:

```
[[2, 1],
 [3, 4]]
```

ğŸ‘‰ Decide if the matrix is invertible or not.

---

### **Q8. Determinant â€“ Transformation**

A transformation matrix is:

```
[[1, 2],
 [2, 4]]
```

ğŸ‘‰ Find its determinant and interpret why it is **not invertible**.

---

### **Q9. Eigenvalues/Eigenvectors â€“ Variance Directions**

For matrix:

```
[[4, 2],
 [1, 3]]
```

ğŸ‘‰ Find eigenvalues and eigenvectors using `np.linalg.eig()`.
ğŸ‘‰ Interpret which direction explains more variance.

---

### **Q10. Eigenvalues â€“ PCA Example**

Suppose a datasetâ€™s covariance matrix is:

```
[[3, 1],
 [1, 2]]
```

ğŸ‘‰ Find eigenvalues and eigenvectors.
ğŸ‘‰ Explain how this could be used in **Principal Component Analysis (PCA)** to reduce dimensions.

---
